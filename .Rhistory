for (t in 1:Time) {
Z[t,] <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
}
create_z <- function(t) {
Z <- matrix(0, nrow = 3, ncol = 1)
Z <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
return(Z)
}
#calculate matrix Q
Q <- matrix(0, nrow = 3, ncol = 3)
for (i in 1:Time) {
Z_t <- create_z(i)
Q <- Q + (Z_t) %*% t(Z_t) * availability * rho * (1 - rho)
}
# Standardized treatment effect (d)
d <- c(0, 0.00964, -0.000172)
d <- c(0, 0.01103, -0.00024)
# Verify the d_bar
d_bar <- 0
for (i in 1:Time) {
d_bar <- d_bar + t(create_z(i)) %*% d / Time
}
print(d_bar)
# Verify the max effect day
effect_by_day <- c()
for (i in 1:(Time/5)) {
effect_by_day <- c(effect_by_day, t(create_z(i*5)) %*% d)
}
print(which.max(effect_by_day))
# Function to calculate sample size for given power and alpha
calculate_sample_size <- function(beta, alpha, d_bar, Q) {
p <- 3
q <- 3
# Calculate power function for given N
power_function <- function(N) {
#calculate the noncentraility parameter
c_N <- N * (t(d) %*% Q %*% d)
df1 <- p
df2 <- N - q - p
# Calculate the critical value for the F-distribution
F_critical <- qf((N - q - p) * (1 - alpha) / (N - q - 1), df1 = df1, df2 = df2)
#F_critical <- qf(1-alpha, df1, df2)
inv.f <- (N - q - 1) * qf(1-alpha,df1,df2) / (N - q - p)
# Evaluate the left-hand side of the formula
calc_power <- pf(((N - q - 1) / (N - q - p)) * F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- pf(p * (N - q - 1) * F_critical / (N - q - p), df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- 1 - pf(F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power = pf(inv.f,df1,df2,ncp = c_N)
print(paste0("Sample size: ", N, ", power: ", calc_power))
return(calc_power - 0.2)
}
# Debugging: Print values at interval endpoints
print(paste("Value at N=10:", power_function(10)))
print(paste("Value at N=100:", power_function(200)))
# Calculate sample size for given power
result <- uniroot(power_function, interval = c(10, 200))
return(ceiling(result$root))
}
# Desired power and significance level
beta <- 0.8
alpha <- 0.05
# Calculate sample size
sample_size <- calculate_sample_size(beta, alpha, d, Q)
print(paste("Required sample size: ", sample_size))
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(MASS)
library(pwr)
# Set parameters
Time <- 210  # Total number of decision points (42 days * 5 times per day)
rho <- 0.4  # Randomization probability
availability <- 0.5  # Constant availability
# Standardized Proximal Main Effect (assuming a quadratic form)
Z <- matrix(0, nrow = Time, ncol = 3)
for (t in 1:Time) {
Z[t,] <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
}
create_z <- function(t) {
Z <- matrix(0, nrow = 3, ncol = 1)
Z <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
return(Z)
}
#calculate matrix Q
Q <- matrix(0, nrow = 3, ncol = 3)
for (i in 1:Time) {
Z_t <- create_z(i)
Q <- Q + (Z_t) %*% t(Z_t) * availability * rho * (1 - rho)
}
# Standardized treatment effect (d)
d <- c(0, 0.00964, -0.000172)
d <- c(0, 0.01, -0.0002)
# Verify the d_bar
d_bar <- 0
for (i in 1:Time) {
d_bar <- d_bar + t(create_z(i)) %*% d / Time
}
print(d_bar)
# Verify the max effect day
effect_by_day <- c()
for (i in 1:(Time/5)) {
effect_by_day <- c(effect_by_day, t(create_z(i*5)) %*% d)
}
print(which.max(effect_by_day))
# Function to calculate sample size for given power and alpha
calculate_sample_size <- function(beta, alpha, d_bar, Q) {
p <- 3
q <- 3
# Calculate power function for given N
power_function <- function(N) {
#calculate the noncentraility parameter
c_N <- N * (t(d) %*% Q %*% d)
df1 <- p
df2 <- N - q - p
# Calculate the critical value for the F-distribution
F_critical <- qf((N - q - p) * (1 - alpha) / (N - q - 1), df1 = df1, df2 = df2)
#F_critical <- qf(1-alpha, df1, df2)
inv.f <- (N - q - 1) * qf(1-alpha,df1,df2) / (N - q - p)
# Evaluate the left-hand side of the formula
calc_power <- pf(((N - q - 1) / (N - q - p)) * F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- pf(p * (N - q - 1) * F_critical / (N - q - p), df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- 1 - pf(F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power = pf(inv.f,df1,df2,ncp = c_N)
print(paste0("Sample size: ", N, ", power: ", calc_power))
return(calc_power - 0.2)
}
# Debugging: Print values at interval endpoints
print(paste("Value at N=10:", power_function(10)))
print(paste("Value at N=100:", power_function(200)))
# Calculate sample size for given power
result <- uniroot(power_function, interval = c(10, 200))
return(ceiling(result$root))
}
# Desired power and significance level
beta <- 0.8
alpha <- 0.05
# Calculate sample size
sample_size <- calculate_sample_size(beta, alpha, d, Q)
print(paste("Required sample size: ", sample_size))
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(MASS)
library(pwr)
# Set parameters
Time <- 210  # Total number of decision points (42 days * 5 times per day)
rho <- 0.4  # Randomization probability
availability <- 0.5  # Constant availability
# Standardized Proximal Main Effect (assuming a quadratic form)
Z <- matrix(0, nrow = Time, ncol = 3)
for (t in 1:Time) {
Z[t,] <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
}
create_z <- function(t) {
Z <- matrix(0, nrow = 3, ncol = 1)
Z <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
return(Z)
}
#calculate matrix Q
Q <- matrix(0, nrow = 3, ncol = 3)
for (i in 1:Time) {
Z_t <- create_z(i)
Q <- Q + (Z_t) %*% t(Z_t) * availability * rho * (1 - rho)
}
# Standardized treatment effect (d)
d <- c(0, 0.00964, -0.000172)
#d <- c(0, 0.01, -0.0002)
# Verify the d_bar
d_bar <- 0
for (i in 1:Time) {
d_bar <- d_bar + t(create_z(i)) %*% d / Time
}
print(d_bar)
# Verify the max effect day
effect_by_day <- c()
for (i in 1:(Time/5)) {
effect_by_day <- c(effect_by_day, t(create_z(i*5)) %*% d)
}
print(which.max(effect_by_day))
# Function to calculate sample size for given power and alpha
calculate_sample_size <- function(beta, alpha, d_bar, Q) {
p <- 3
q <- 3
# Calculate power function for given N
power_function <- function(N) {
#calculate the noncentraility parameter
c_N <- N * (t(d) %*% Q %*% d)
df1 <- p
df2 <- N - q - p
# Calculate the critical value for the F-distribution
F_critical <- qf((N - q - p) * (1 - alpha) / (N - q - 1), df1 = df1, df2 = df2)
#F_critical <- qf(1-alpha, df1, df2)
inv.f <- (N - q - 1) * qf(1-alpha,df1,df2) / (N - q - p)
# Evaluate the left-hand side of the formula
calc_power <- pf(((N - q - 1) / (N - q - p)) * F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- pf(p * (N - q - 1) * F_critical / (N - q - p), df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- 1 - pf(F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power = pf(inv.f,df1,df2,ncp = c_N)
print(paste0("Sample size: ", N, ", power: ", calc_power))
return(calc_power - 0.2)
}
# Debugging: Print values at interval endpoints
print(paste("Value at N=10:", power_function(10)))
print(paste("Value at N=100:", power_function(200)))
# Calculate sample size for given power
result <- uniroot(power_function, interval = c(10, 200))
return(ceiling(result$root))
}
# Desired power and significance level
beta <- 0.8
alpha <- 0.05
# Calculate sample size
sample_size <- calculate_sample_size(beta, alpha, d, Q)
print(paste("Required sample size: ", sample_size))
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(MASS)
library(pwr)
# Set parameters
Time <- 210  # Total number of decision points (42 days * 5 times per day)
rho <- 0.4  # Randomization probability
availability <- 0.5  # Constant availability
# Standardized Proximal Main Effect (assuming a quadratic form)
Z <- matrix(0, nrow = Time, ncol = 3)
for (t in 1:Time) {
Z[t,] <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
}
create_z <- function(t) {
Z <- matrix(0, nrow = 3, ncol = 1)
Z <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
return(Z)
}
#calculate matrix Q
Q <- matrix(0, nrow = 3, ncol = 3)
for (i in 1:Time) {
Z_t <- create_z(i)
Q <- Q + (Z_t) %*% t(Z_t) * availability * rho * (1 - rho)
}
# Standardized treatment effect (d)
d <- c(0, 0.00964, -0.000172)
#d <- c(0, 0.01, -0.0002)
# Verify the d_bar
d_bar <- 0
for (i in 1:Time) {
d_bar <- d_bar + t(create_z(i)) %*% d / Time
}
print(d_bar)
# Verify the max effect day
effect_by_day <- c()
for (i in 1:(Time/5)) {
effect_by_day <- c(effect_by_day, t(create_z(i*5)) %*% d)
}
print(which.max(effect_by_day))
# Function to calculate sample size for given power and alpha
calculate_sample_size <- function(beta, alpha, d_bar, Q) {
p <- 3
q <- 3
# Calculate power function for given N
power_function <- function(N) {
#calculate the noncentraility parameter
c_N <- N * (t(d) %*% Q %*% d)
df1 <- p
df2 <- N - q - p
# Calculate the critical value for the F-distribution
F_critical <- qf((N - q - 1) * (1 - alpha) / (N - q - p), df1 = df1, df2 = df2)
#F_critical <- qf(1-alpha, df1, df2)
inv.f <- (N - q - 1) * qf(1-alpha,df1,df2) / (N - q - p)
# Evaluate the left-hand side of the formula
calc_power <- pf(((N - q - p) / (N - q - 1)) * F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- pf(p * (N - q - 1) * F_critical / (N - q - p), df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- 1 - pf(F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power = pf(inv.f,df1,df2,ncp = c_N)
print(paste0("Sample size: ", N, ", power: ", calc_power))
return(calc_power - 0.2)
}
# Debugging: Print values at interval endpoints
print(paste("Value at N=10:", power_function(10)))
print(paste("Value at N=100:", power_function(200)))
# Calculate sample size for given power
result <- uniroot(power_function, interval = c(10, 200))
return(ceiling(result$root))
}
# Desired power and significance level
beta <- 0.8
alpha <- 0.05
# Calculate sample size
sample_size <- calculate_sample_size(beta, alpha, d, Q)
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(MASS)
library(pwr)
# Set parameters
Time <- 210  # Total number of decision points (42 days * 5 times per day)
rho <- 0.4  # Randomization probability
availability <- 0.5  # Constant availability
# Standardized Proximal Main Effect (assuming a quadratic form)
Z <- matrix(0, nrow = Time, ncol = 3)
for (t in 1:Time) {
Z[t,] <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
}
create_z <- function(t) {
Z <- matrix(0, nrow = 3, ncol = 1)
Z <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
return(Z)
}
#calculate matrix Q
Q <- matrix(0, nrow = 3, ncol = 3)
for (i in 1:Time) {
Z_t <- create_z(i)
Q <- Q + (Z_t) %*% t(Z_t) * availability * rho * (1 - rho)
}
# Standardized treatment effect (d)
d <- c(0, 0.00964, -0.000172)
#d <- c(0, 0.01, -0.0002)
# Verify the d_bar
d_bar <- 0
for (i in 1:Time) {
d_bar <- d_bar + t(create_z(i)) %*% d / Time
}
print(d_bar)
# Verify the max effect day
effect_by_day <- c()
for (i in 1:(Time/5)) {
effect_by_day <- c(effect_by_day, t(create_z(i*5)) %*% d)
}
print(which.max(effect_by_day))
# Function to calculate sample size for given power and alpha
calculate_sample_size <- function(beta, alpha, d_bar, Q) {
p <- 3
q <- 3
# Calculate power function for given N
power_function <- function(N) {
#calculate the noncentraility parameter
c_N <- N * (t(d) %*% Q %*% d)
df1 <- p
df2 <- N - q - p
# Calculate the critical value for the F-distribution
F_critical <- qf((N - q - 1) * p * (1 - alpha) / (N - q - p), df1 = df1, df2 = df2)
#F_critical <- qf(1-alpha, df1, df2)
inv.f <- (N - q - 1) * qf(1-alpha,df1,df2) / (N - q - p)
# Evaluate the left-hand side of the formula
calc_power <- pf(((N - q - p) / (N - q - 1) / q) * F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- pf(p * (N - q - 1) * F_critical / (N - q - p), df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- 1 - pf(F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power = pf(inv.f,df1,df2,ncp = c_N)
print(paste0("Sample size: ", N, ", power: ", calc_power))
return(calc_power - 0.2)
}
# Debugging: Print values at interval endpoints
print(paste("Value at N=10:", power_function(10)))
print(paste("Value at N=100:", power_function(200)))
# Calculate sample size for given power
result <- uniroot(power_function, interval = c(10, 200))
return(ceiling(result$root))
}
# Desired power and significance level
beta <- 0.8
alpha <- 0.05
# Calculate sample size
sample_size <- calculate_sample_size(beta, alpha, d, Q)
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(MASS)
library(pwr)
# Set parameters
Time <- 210  # Total number of decision points (42 days * 5 times per day)
rho <- 0.4  # Randomization probability
availability <- 0.5  # Constant availability
# Standardized Proximal Main Effect (assuming a quadratic form)
Z <- matrix(0, nrow = Time, ncol = 3)
for (t in 1:Time) {
Z[t,] <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
}
create_z <- function(t) {
Z <- matrix(0, nrow = 3, ncol = 1)
Z <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
return(Z)
}
#calculate matrix Q
Q <- matrix(0, nrow = 3, ncol = 3)
for (i in 1:Time) {
Z_t <- create_z(i)
Q <- Q + (Z_t) %*% t(Z_t) * availability * rho * (1 - rho)
}
# Standardized treatment effect (d)
d <- c(0, 0.00964, -0.000172)
d <- c(0, 0.01, -0.0002)
# Verify the d_bar
d_bar <- 0
for (i in 1:Time) {
d_bar <- d_bar + t(create_z(i)) %*% d / Time
}
print(d_bar)
# Verify the max effect day
effect_by_day <- c()
for (i in 1:(Time/5)) {
effect_by_day <- c(effect_by_day, t(create_z(i*5)) %*% d)
}
print(which.max(effect_by_day))
# Function to calculate sample size for given power and alpha
calculate_sample_size <- function(beta, alpha, d_bar, Q) {
p <- 3
q <- 3
# Calculate power function for given N
power_function <- function(N) {
#calculate the noncentraility parameter
c_N <- N * (t(d) %*% Q %*% d)
df1 <- p
df2 <- N - q - p
# Calculate the critical value for the F-distribution
F_critical <- qf((N - q - p) * (1 - alpha) / (N - q - 1), df1 = df1, df2 = df2)
#F_critical <- qf(1-alpha, df1, df2)
inv.f <- (N - q - 1) * qf(1-alpha,df1,df2) / (N - q - p)
# Evaluate the left-hand side of the formula
calc_power <- pf(((N - q - 1) / (N - q - p)) * F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- pf(p * (N - q - 1) * F_critical / (N - q - p), df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- 1 - pf(F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power = pf(inv.f,df1,df2,ncp = c_N)
print(paste0("Sample size: ", N, ", power: ", calc_power))
return(calc_power - 0.2)
}
# Debugging: Print values at interval endpoints
print(paste("Value at N=10:", power_function(10)))
print(paste("Value at N=100:", power_function(200)))
# Calculate sample size for given power
result <- uniroot(power_function, interval = c(10, 200))
return(ceiling(result$root))
}
# Desired power and significance level
beta <- 0.8
alpha <- 0.05
# Calculate sample size
sample_size <- calculate_sample_size(beta, alpha, d, Q)
print(paste("Required sample size: ", sample_size))
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(MASS)
library(pwr)
# Set parameters
Time <- 210  # Total number of decision points (42 days * 5 times per day)
rho <- 0.4  # Randomization probability
availability <- 0.5  # Constant availability
# Standardized Proximal Main Effect (assuming a quadratic form)
Z <- matrix(0, nrow = Time, ncol = 3)
for (t in 1:Time) {
Z[t,] <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
}
create_z <- function(t) {
Z <- matrix(0, nrow = 3, ncol = 1)
Z <- c(1, floor((t-1)/5), floor((t-1)/5)^2)
return(Z)
}
#calculate matrix Q
Q <- matrix(0, nrow = 3, ncol = 3)
for (i in 1:Time) {
Z_t <- create_z(i)
Q <- Q + (Z_t) %*% t(Z_t) * availability * rho * (1 - rho)
}
# Standardized treatment effect (d)
d <- c(0, 0.00964, -0.000172)
#d <- c(0, 0.01, -0.0002)
# Verify the d_bar
d_bar <- 0
for (i in 1:Time) {
d_bar <- d_bar + t(create_z(i)) %*% d / Time
}
print(d_bar)
# Verify the max effect day
effect_by_day <- c()
for (i in 1:(Time/5)) {
effect_by_day <- c(effect_by_day, t(create_z(i*5)) %*% d)
}
print(which.max(effect_by_day))
# Function to calculate sample size for given power and alpha
calculate_sample_size <- function(beta, alpha, d_bar, Q) {
p <- 3
q <- 3
# Calculate power function for given N
power_function <- function(N) {
#calculate the noncentraility parameter
c_N <- N * (t(d) %*% Q %*% d)
df1 <- p
df2 <- N - q - p
# Calculate the critical value for the F-distribution
F_critical <- qf((N - q - p) * (1 - alpha) / (N - q - 1), df1 = df1, df2 = df2)
#F_critical <- qf(1-alpha, df1, df2)
inv.f <- (N - q - 1) * qf(1-alpha,df1,df2) / (N - q - p)
# Evaluate the left-hand side of the formula
calc_power <- pf(((N - q - 1) / (N - q - p)) * F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- pf(p * (N - q - 1) * F_critical / (N - q - p), df1 = df1, df2 = df2, ncp = c_N)
#calc_power <- 1 - pf(F_critical, df1 = df1, df2 = df2, ncp = c_N)
#calc_power = pf(inv.f,df1,df2,ncp = c_N)
print(paste0("Sample size: ", N, ", power: ", calc_power))
return(calc_power - 0.2)
}
# Debugging: Print values at interval endpoints
print(paste("Value at N=10:", power_function(10)))
print(paste("Value at N=100:", power_function(200)))
# Calculate sample size for given power
result <- uniroot(power_function, interval = c(10, 200))
return(ceiling(result$root))
}
# Desired power and significance level
beta <- 0.8
alpha <- 0.05
# Calculate sample size
sample_size <- calculate_sample_size(beta, alpha, d, Q)
print(paste("Required sample size: ", sample_size))
